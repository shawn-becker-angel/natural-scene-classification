{"cells":[{"cell_type":"markdown","metadata":{},"source":[" **Note:** Igonre or comment jovian lines if you are running this notebook."]},{"cell_type":"code","execution_count":2,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2022-05-15T14:59:54.633875Z","iopub.status.busy":"2022-05-15T14:59:54.633166Z","iopub.status.idle":"2022-05-15T14:59:58.715978Z","shell.execute_reply":"2022-05-15T14:59:58.714806Z","shell.execute_reply.started":"2022-05-15T14:59:54.63383Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["./kaggle/input/seg_test/forest/23933.jpg\n","./kaggle/input/seg_test/buildings/20553.jpg\n","./kaggle/input/seg_test/glacier/21842.jpg\n","./kaggle/input/seg_test/street/22387.jpg\n","./kaggle/input/seg_test/mountain/20584.jpg\n","./kaggle/input/seg_test/sea/22393.jpg\n","./kaggle/input/seg_train/forest/5109.jpg\n","./kaggle/input/seg_train/buildings/12536.jpg\n","./kaggle/input/seg_train/glacier/4217.jpg\n","./kaggle/input/seg_train/street/9733.jpg\n","./kaggle/input/seg_train/mountain/14147.jpg\n","./kaggle/input/seg_train/sea/6400.jpg\n","./kaggle/input/seg_pred/63.jpg\n","done\n"]}],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('./kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","        break\n","print(\"done\")\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"code","execution_count":3,"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","execution":{"iopub.execute_input":"2022-05-15T15:00:32.202931Z","iopub.status.busy":"2022-05-15T15:00:32.202439Z","iopub.status.idle":"2022-05-15T15:00:33.630943Z","shell.execute_reply":"2022-05-15T15:00:33.629497Z","shell.execute_reply.started":"2022-05-15T15:00:32.202882Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple/\n","Requirement already satisfied: pip in ./venv/lib/python3.9/site-packages (22.1)\n","Looking in indexes: https://pypi.org/simple/\n","Requirement already satisfied: torch in ./venv/lib/python3.9/site-packages (1.11.0)\n","Requirement already satisfied: typing-extensions in ./venv/lib/python3.9/site-packages (from torch) (4.2.0)\n","Note: you may need to restart the kernel to use updated packages.\n","Looking in indexes: https://pypi.org/simple/\n","Requirement already satisfied: torchvision in ./venv/lib/python3.9/site-packages (0.12.0)\n","Requirement already satisfied: torch==1.11.0 in ./venv/lib/python3.9/site-packages (from torchvision) (1.11.0)\n","Requirement already satisfied: typing-extensions in ./venv/lib/python3.9/site-packages (from torchvision) (4.2.0)\n","Requirement already satisfied: numpy in ./venv/lib/python3.9/site-packages (from torchvision) (1.22.3)\n","Requirement already satisfied: requests in ./venv/lib/python3.9/site-packages (from torchvision) (2.27.1)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in ./venv/lib/python3.9/site-packages (from torchvision) (9.1.0)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./venv/lib/python3.9/site-packages (from requests->torchvision) (1.26.9)\n","Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.9/site-packages (from requests->torchvision) (3.3)\n","Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.9/site-packages (from requests->torchvision) (2021.10.8)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in ./venv/lib/python3.9/site-packages (from requests->torchvision) (2.0.12)\n","Note: you may need to restart the kernel to use updated packages.\n","Looking in indexes: https://pypi.org/simple/\n","Requirement already satisfied: matplotlib in ./venv/lib/python3.9/site-packages (3.5.2)\n","Requirement already satisfied: numpy>=1.17 in ./venv/lib/python3.9/site-packages (from matplotlib) (1.22.3)\n","Requirement already satisfied: pillow>=6.2.0 in ./venv/lib/python3.9/site-packages (from matplotlib) (9.1.0)\n","Requirement already satisfied: python-dateutil>=2.7 in ./venv/lib/python3.9/site-packages (from matplotlib) (2.8.2)\n","Requirement already satisfied: kiwisolver>=1.0.1 in ./venv/lib/python3.9/site-packages (from matplotlib) (1.4.2)\n","Requirement already satisfied: cycler>=0.10 in ./venv/lib/python3.9/site-packages (from matplotlib) (0.11.0)\n","Requirement already satisfied: fonttools>=4.22.0 in ./venv/lib/python3.9/site-packages (from matplotlib) (4.33.3)\n","Requirement already satisfied: pyparsing>=2.2.1 in ./venv/lib/python3.9/site-packages (from matplotlib) (3.0.9)\n","Requirement already satisfied: packaging>=20.0 in ./venv/lib/python3.9/site-packages (from matplotlib) (21.3)\n","Requirement already satisfied: six>=1.5 in ./venv/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n","Note: you may need to restart the kernel to use updated packages.\n","done\n"]}],"source":["!python3 -m pip install --upgrade pip\n","%pip install torch\n","%pip install torchvision\n","%pip install matplotlib\n","print(\"done\")"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2022-05-15T11:28:16.578152Z","iopub.status.busy":"2022-05-15T11:28:16.577587Z","iopub.status.idle":"2022-05-15T11:28:18.072361Z","shell.execute_reply":"2022-05-15T11:28:18.071127Z","shell.execute_reply.started":"2022-05-15T11:28:16.57809Z"},"trusted":true},"outputs":[],"source":["#import necessory libraries\n","import torch\n","import torchvision\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torchvision import transforms\n","from torchvision.datasets import ImageFolder\n","from torchvision.utils import make_grid\n","from torch.utils.data.dataloader import DataLoader\n","from torch.utils.data import random_split\n","%matplotlib inline"]},{"cell_type":"markdown","metadata":{},"source":["> # **Exploring the Dataset**"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2022-05-15T11:28:18.074059Z","iopub.status.busy":"2022-05-15T11:28:18.073698Z","iopub.status.idle":"2022-05-15T11:28:18.079201Z","shell.execute_reply":"2022-05-15T11:28:18.078244Z","shell.execute_reply.started":"2022-05-15T11:28:18.074017Z"},"trusted":true},"outputs":[],"source":["#project name\n","project_name = 'natural-scene-classification'"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2022-05-15T11:28:18.082527Z","iopub.status.busy":"2022-05-15T11:28:18.082127Z","iopub.status.idle":"2022-05-15T11:28:18.090462Z","shell.execute_reply":"2022-05-15T11:28:18.089409Z","shell.execute_reply.started":"2022-05-15T11:28:18.082486Z"},"trusted":true},"outputs":[],"source":["data_dir = \"./kaggle/input/seg_train/\"\n","test_data_dir = \"./kaggle/input/seg_test\""]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2022-05-15T11:28:18.095274Z","iopub.status.busy":"2022-05-15T11:28:18.094843Z","iopub.status.idle":"2022-05-15T11:28:18.384462Z","shell.execute_reply":"2022-05-15T11:28:18.383058Z","shell.execute_reply.started":"2022-05-15T11:28:18.095237Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of Buildings : 2191\n","['12536.jpg', '16750.jpg', '16988.jpg', '4571.jpg', '12244.jpg']\n"]}],"source":["\n","building_files = os.listdir(data_dir + '/buildings')\n","print(f\"Number of Buildings : {len(building_files)}\")\n","print(building_files[:5])"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2022-05-15T11:28:18.38636Z","iopub.status.busy":"2022-05-15T11:28:18.386032Z","iopub.status.idle":"2022-05-15T11:28:31.882703Z","shell.execute_reply":"2022-05-15T11:28:31.881529Z","shell.execute_reply.started":"2022-05-15T11:28:18.386328Z"},"trusted":true},"outputs":[],"source":["dataset = ImageFolder(data_dir,transform = transforms.Compose([\n","    transforms.Resize((150,150)),transforms.ToTensor()\n","]))\n","test_dataset = ImageFolder(test_data_dir,transforms.Compose([\n","    transforms.Resize((150,150)),transforms.ToTensor()\n","]))"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2022-05-15T11:28:31.884376Z","iopub.status.busy":"2022-05-15T11:28:31.884093Z","iopub.status.idle":"2022-05-15T11:28:31.960478Z","shell.execute_reply":"2022-05-15T11:28:31.95934Z","shell.execute_reply.started":"2022-05-15T11:28:31.884345Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([3, 150, 150]) 0\n"]}],"source":["img, label = dataset[0]\n","print(img.shape,label)"]},{"cell_type":"markdown","metadata":{},"source":["The images are rgb and have 150*150 pixel in each image"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2022-05-15T11:28:31.963453Z","iopub.status.busy":"2022-05-15T11:28:31.963114Z","iopub.status.idle":"2022-05-15T11:28:31.969331Z","shell.execute_reply":"2022-05-15T11:28:31.968444Z","shell.execute_reply.started":"2022-05-15T11:28:31.963419Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Images in tarining data : 14034\n","Images in test data : 3000\n"]}],"source":["print(f\"Images in training data : {len(dataset)}\")\n","print(f\"Images in test data : {len(test_dataset)}\")"]},{"cell_type":"markdown","metadata":{},"source":["We have 14034 images in training data and 3000 images in test dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-15T11:28:31.971058Z","iopub.status.busy":"2022-05-15T11:28:31.97054Z","iopub.status.idle":"2022-05-15T11:28:31.982722Z","shell.execute_reply":"2022-05-15T11:28:31.981891Z","shell.execute_reply.started":"2022-05-15T11:28:31.971015Z"},"trusted":true},"outputs":[],"source":["print(\"Following classes are there : \\n\",dataset.classes)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-15T11:28:31.984434Z","iopub.status.busy":"2022-05-15T11:28:31.984005Z","iopub.status.idle":"2022-05-15T11:28:31.992422Z","shell.execute_reply":"2022-05-15T11:28:31.991509Z","shell.execute_reply.started":"2022-05-15T11:28:31.984399Z"},"trusted":true},"outputs":[],"source":["def display_img(img,label):\n","    print(f\"Label : {dataset.classes[label]}\")\n","    plt.imshow(img.permute(1,2,0))"]},{"cell_type":"markdown","metadata":{},"source":["> # Visualising some images :"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-15T11:28:31.994197Z","iopub.status.busy":"2022-05-15T11:28:31.9937Z","iopub.status.idle":"2022-05-15T11:28:32.227772Z","shell.execute_reply":"2022-05-15T11:28:32.226791Z","shell.execute_reply.started":"2022-05-15T11:28:31.994157Z"},"trusted":true},"outputs":[],"source":["display_img(*dataset[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-15T11:28:32.229621Z","iopub.status.busy":"2022-05-15T11:28:32.2293Z","iopub.status.idle":"2022-05-15T11:28:32.440555Z","shell.execute_reply":"2022-05-15T11:28:32.439435Z","shell.execute_reply.started":"2022-05-15T11:28:32.229589Z"},"trusted":true},"outputs":[],"source":["display_img(*dataset[5000])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-15T11:28:32.443025Z","iopub.status.busy":"2022-05-15T11:28:32.442553Z","iopub.status.idle":"2022-05-15T11:28:32.644801Z","shell.execute_reply":"2022-05-15T11:28:32.643956Z","shell.execute_reply.started":"2022-05-15T11:28:32.442971Z"},"trusted":true},"outputs":[],"source":["display_img(*dataset[8000])"]},{"cell_type":"markdown","metadata":{},"source":["> # Training and Validation Datasets : "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-15T11:28:32.646632Z","iopub.status.busy":"2022-05-15T11:28:32.646139Z","iopub.status.idle":"2022-05-15T11:28:32.656188Z","shell.execute_reply":"2022-05-15T11:28:32.655284Z","shell.execute_reply.started":"2022-05-15T11:28:32.646588Z"},"trusted":true},"outputs":[],"source":["random_seed = 2021\n","torch.manual_seed(random_seed)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-15T11:28:32.658325Z","iopub.status.busy":"2022-05-15T11:28:32.657751Z","iopub.status.idle":"2022-05-15T11:28:32.676039Z","shell.execute_reply":"2022-05-15T11:28:32.675048Z","shell.execute_reply.started":"2022-05-15T11:28:32.658274Z"},"trusted":true},"outputs":[],"source":["val_size = 2000\n","train_size = len(dataset) - val_size \n","\n","train_data,val_data = random_split(dataset,[train_size,val_size])\n","print(f\"Length of Train Data : {len(train_data)}\")\n","print(f\"Length of Validation Data : {len(val_data)}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-15T11:28:32.678306Z","iopub.status.busy":"2022-05-15T11:28:32.677727Z","iopub.status.idle":"2022-05-15T11:28:32.682703Z","shell.execute_reply":"2022-05-15T11:28:32.681973Z","shell.execute_reply.started":"2022-05-15T11:28:32.678262Z"},"trusted":true},"outputs":[],"source":["batch_size = 128"]},{"cell_type":"markdown","metadata":{},"source":["> # Load the dataset into batches:"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-15T11:28:32.685347Z","iopub.status.busy":"2022-05-15T11:28:32.684818Z","iopub.status.idle":"2022-05-15T11:28:32.697062Z","shell.execute_reply":"2022-05-15T11:28:32.696044Z","shell.execute_reply.started":"2022-05-15T11:28:32.685309Z"},"trusted":true},"outputs":[],"source":["train_dl = DataLoader(train_data, batch_size, shuffle = True, num_workers = 4, pin_memory = True)\n","val_dl = DataLoader(val_data, batch_size*2, num_workers = 4, pin_memory = True)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-15T11:28:32.698887Z","iopub.status.busy":"2022-05-15T11:28:32.698548Z","iopub.status.idle":"2022-05-15T11:28:32.7081Z","shell.execute_reply":"2022-05-15T11:28:32.707152Z","shell.execute_reply.started":"2022-05-15T11:28:32.698853Z"},"trusted":true},"outputs":[],"source":["from torchvision.utils import make_grid\n","\n","def show_batch(dl):\n","    for images, labels in dl:\n","        fig,ax = plt.subplots(figsize = (16,12))\n","        ax.set_xticks([])\n","        ax.set_yticks([])\n","        ax.imshow(make_grid(images,nrow=16).permute(1,2,0))\n","        break"]},{"cell_type":"markdown","metadata":{},"source":["> # **Grid Of Train Data Images :**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-15T11:28:32.71002Z","iopub.status.busy":"2022-05-15T11:28:32.709623Z","iopub.status.idle":"2022-05-15T11:28:35.586299Z","shell.execute_reply":"2022-05-15T11:28:35.584675Z","shell.execute_reply.started":"2022-05-15T11:28:32.709983Z"},"trusted":true},"outputs":[],"source":["show_batch(train_dl)"]},{"cell_type":"markdown","metadata":{},"source":["> # Base Model for Image Classification:"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-15T11:28:35.589076Z","iopub.status.busy":"2022-05-15T11:28:35.588567Z","iopub.status.idle":"2022-05-15T11:28:35.609517Z","shell.execute_reply":"2022-05-15T11:28:35.608037Z","shell.execute_reply.started":"2022-05-15T11:28:35.589023Z"},"trusted":true},"outputs":[],"source":["class ImageClassificationBase(nn.Module):\n","    \n","    def training_step(self, batch):\n","        images, labels = batch \n","        out = self(images)                  # Generate predictions\n","        loss = F.cross_entropy(out, labels) # Calculate loss\n","        return loss\n","    \n","    def validation_step(self, batch):\n","        images, labels = batch \n","        out = self(images)                    # Generate predictions\n","        loss = F.cross_entropy(out, labels)   # Calculate loss\n","        acc = accuracy(out, labels)           # Calculate accuracy\n","        return {'val_loss': loss.detach(), 'val_acc': acc}\n","        \n","    def validation_epoch_end(self, outputs):\n","        batch_losses = [x['val_loss'] for x in outputs]\n","        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n","        batch_accs = [x['val_acc'] for x in outputs]\n","        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n","        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n","    \n","    def epoch_end(self, epoch, result):\n","        print(\"Epoch [{}], train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n","            epoch, result['train_loss'], result['val_loss'], result['val_acc']))\n","        \n","def accuracy(outputs, labels):\n","    _, preds = torch.max(outputs, dim=1)\n","    return torch.tensor(torch.sum(preds == labels).item() / len(preds))"]},{"cell_type":"markdown","metadata":{},"source":["# Netural Scene Classfication Model:"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-15T11:28:35.611253Z","iopub.status.busy":"2022-05-15T11:28:35.610796Z","iopub.status.idle":"2022-05-15T11:28:35.626472Z","shell.execute_reply":"2022-05-15T11:28:35.625552Z","shell.execute_reply.started":"2022-05-15T11:28:35.611216Z"},"trusted":true},"outputs":[],"source":["class NaturalSceneClassification(ImageClassificationBase):\n","    \n","    def __init__(self):\n","        \n","        super().__init__()\n","        self.network = nn.Sequential(\n","            \n","            nn.Conv2d(3, 32, kernel_size = 3, padding = 1),\n","            nn.ReLU(),\n","            nn.Conv2d(32,64, kernel_size = 3, stride = 1, padding = 1),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2,2),\n","        \n","            nn.Conv2d(64, 128, kernel_size = 3, stride = 1, padding = 1),\n","            nn.ReLU(),\n","            nn.Conv2d(128 ,128, kernel_size = 3, stride = 1, padding = 1),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2,2),\n","            \n","            nn.Conv2d(128, 256, kernel_size = 3, stride = 1, padding = 1),\n","            nn.ReLU(),\n","            nn.Conv2d(256,256, kernel_size = 3, stride = 1, padding = 1),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2,2),\n","            \n","            nn.Flatten(),\n","            nn.Linear(82944,1024),\n","            nn.ReLU(),\n","            nn.Linear(1024, 512),\n","            nn.ReLU(),\n","            nn.Linear(512,6)\n","        )\n","    \n","    def forward(self, xb):\n","        return self.network(xb)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-15T11:28:35.62807Z","iopub.status.busy":"2022-05-15T11:28:35.627654Z","iopub.status.idle":"2022-05-15T11:28:36.445817Z","shell.execute_reply":"2022-05-15T11:28:36.444543Z","shell.execute_reply.started":"2022-05-15T11:28:35.62803Z"},"trusted":true},"outputs":[],"source":["model = NaturalSceneClassification()\n","model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-15T11:28:36.448471Z","iopub.status.busy":"2022-05-15T11:28:36.447983Z","iopub.status.idle":"2022-05-15T11:28:50.582271Z","shell.execute_reply":"2022-05-15T11:28:50.581126Z","shell.execute_reply.started":"2022-05-15T11:28:36.448421Z"},"trusted":true},"outputs":[],"source":["for images, labels in train_dl:\n","    print('images.shape:', images.shape)\n","    out = model(images)\n","    print('out.shape:', out.shape)\n","    print('out[0]:', out[0])\n","    break"]},{"cell_type":"markdown","metadata":{},"source":["Helper Function or classes to Load Data into GPU"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-15T11:28:50.584704Z","iopub.status.busy":"2022-05-15T11:28:50.584118Z","iopub.status.idle":"2022-05-15T11:28:50.599699Z","shell.execute_reply":"2022-05-15T11:28:50.598438Z","shell.execute_reply.started":"2022-05-15T11:28:50.584656Z"},"trusted":true},"outputs":[],"source":["def get_default_device():\n","    \"\"\" Set Device to GPU or CPU\"\"\"\n","    if torch.cuda.is_available():\n","        return torch.device('cuda')\n","    else:\n","        return torch.device('cpu')\n","    \n","\n","def to_device(data, device):\n","    \"Move data to the device\"\n","    if isinstance(data,(list,tuple)):\n","        return [to_device(x,device) for x in data]\n","    return data.to(device,non_blocking = True)\n","\n","class DeviceDataLoader():\n","    \"\"\" Wrap a dataloader to move data to a device \"\"\"\n","    \n","    def __init__(self, dl, device):\n","        self.dl = dl\n","        self.device = device\n","    \n","    def __iter__(self):\n","        \"\"\" Yield a batch of data after moving it to device\"\"\"\n","        for b in self.dl:\n","            yield to_device(b,self.device)\n","            \n","    def __len__(self):\n","        \"\"\" Number of batches \"\"\"\n","        return len(self.dl)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-15T11:28:50.602259Z","iopub.status.busy":"2022-05-15T11:28:50.601524Z","iopub.status.idle":"2022-05-15T11:28:50.621706Z","shell.execute_reply":"2022-05-15T11:28:50.620662Z","shell.execute_reply.started":"2022-05-15T11:28:50.60221Z"},"trusted":true},"outputs":[],"source":["device = get_default_device()\n","device"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-15T11:28:50.624224Z","iopub.status.busy":"2022-05-15T11:28:50.623417Z","iopub.status.idle":"2022-05-15T11:28:50.638976Z","shell.execute_reply":"2022-05-15T11:28:50.637798Z","shell.execute_reply.started":"2022-05-15T11:28:50.624165Z"},"trusted":true},"outputs":[],"source":["# load the into GPU\n","train_dl = DeviceDataLoader(train_dl, device)\n","val_dl = DeviceDataLoader(val_dl, device)\n","to_device(model, device)"]},{"cell_type":"markdown","metadata":{},"source":["> # **Model Fitting**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-15T11:28:50.641577Z","iopub.status.busy":"2022-05-15T11:28:50.640816Z","iopub.status.idle":"2022-05-15T11:28:50.655246Z","shell.execute_reply":"2022-05-15T11:28:50.654023Z","shell.execute_reply.started":"2022-05-15T11:28:50.641525Z"},"trusted":true},"outputs":[],"source":["@torch.no_grad()\n","def evaluate(model, val_loader):\n","    model.eval()\n","    outputs = [model.validation_step(batch) for batch in val_loader]\n","    return model.validation_epoch_end(outputs)\n","\n","def fit(epochs, lr, model, train_loader, val_loader, opt_func = torch.optim.SGD):\n","    \n","    history = []\n","    optimizer = opt_func(model.parameters(),lr)\n","    for epoch in range(epochs):\n","        \n","        model.train()\n","        train_losses = []\n","        for batch in train_loader:\n","            loss = model.training_step(batch)\n","            train_losses.append(loss)\n","            loss.backward()\n","            optimizer.step()\n","            optimizer.zero_grad()\n","            \n","        result = evaluate(model, val_loader)\n","        result['train_loss'] = torch.stack(train_losses).mean().item()\n","        model.epoch_end(epoch, result)\n","        history.append(result)\n","    \n","    return history"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-15T11:28:50.663723Z","iopub.status.busy":"2022-05-15T11:28:50.66275Z","iopub.status.idle":"2022-05-15T11:28:51.472451Z","shell.execute_reply":"2022-05-15T11:28:51.471301Z","shell.execute_reply.started":"2022-05-15T11:28:50.663665Z"},"trusted":true},"outputs":[],"source":["#load the model to the device\n","model = to_device(NaturalSceneClassification(),device)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-15T11:28:51.474617Z","iopub.status.busy":"2022-05-15T11:28:51.474076Z","iopub.status.idle":"2022-05-15T11:31:47.993531Z","shell.execute_reply":"2022-05-15T11:31:47.992369Z","shell.execute_reply.started":"2022-05-15T11:28:51.474578Z"},"trusted":true},"outputs":[],"source":["#initial evaluation of the model\n","evaluate(model,val_dl)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-15T11:31:47.995685Z","iopub.status.busy":"2022-05-15T11:31:47.995314Z","iopub.status.idle":"2022-05-15T11:31:48.000538Z","shell.execute_reply":"2022-05-15T11:31:47.999635Z","shell.execute_reply.started":"2022-05-15T11:31:47.995641Z"},"trusted":true},"outputs":[],"source":["#set the no. of epochs, optimizer funtion and learning rate\n","num_epochs = 30\n","opt_func = torch.optim.Adam\n","lr = 0.001"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-15T11:31:48.002705Z","iopub.status.busy":"2022-05-15T11:31:48.002182Z","iopub.status.idle":"2022-05-15T11:46:11.265813Z","shell.execute_reply":"2022-05-15T11:46:11.263234Z","shell.execute_reply.started":"2022-05-15T11:31:48.002667Z"},"trusted":true},"outputs":[],"source":["#fitting the model on training data and record the result after each epoch\n","history = fit(num_epochs, lr, model, train_dl, val_dl, opt_func)"]},{"cell_type":"markdown","metadata":{},"source":["> # Graphs for Model Accuracy and Losses :"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2022-05-15T11:46:11.268032Z","iopub.status.idle":"2022-05-15T11:46:11.268836Z"},"trusted":true},"outputs":[],"source":["def plot_accuracies(history):\n","    \"\"\" Plot the history of accuracies\"\"\"\n","    accuracies = [x['val_acc'] for x in history]\n","    plt.plot(accuracies, '-x')\n","    plt.xlabel('epoch')\n","    plt.ylabel('accuracy')\n","    plt.title('Accuracy vs. No. of epochs');\n","    \n","\n","plot_accuracies(history)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2022-05-15T11:46:11.270823Z","iopub.status.idle":"2022-05-15T11:46:11.271855Z"},"trusted":true},"outputs":[],"source":["def plot_losses(history):\n","    \"\"\" Plot the losses in each epoch\"\"\"\n","    train_losses = [x.get('train_loss') for x in history]\n","    val_losses = [x['val_loss'] for x in history]\n","    plt.plot(train_losses, '-bx')\n","    plt.plot(val_losses, '-rx')\n","    plt.xlabel('epoch')\n","    plt.ylabel('loss')\n","    plt.legend(['Training', 'Validation'])\n","    plt.title('Loss vs. No. of epochs');\n","\n","plot_losses(history)"]},{"cell_type":"markdown","metadata":{},"source":["> # Evaluate Test Data :"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2022-05-15T11:46:11.273726Z","iopub.status.idle":"2022-05-15T11:46:11.27482Z"},"trusted":true},"outputs":[],"source":["# Apply the model on test dataset and Get the results\n","test_loader = DeviceDataLoader(DataLoader(test_dataset, batch_size*2), device)\n","result = evaluate(model, test_loader)\n","result"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2022-05-15T11:46:11.276627Z","iopub.status.idle":"2022-05-15T11:46:11.277687Z"},"trusted":true},"outputs":[],"source":["#save the model\n","torch.save(model.state_dict(), 'natural-scene-classification.pth')"]},{"cell_type":"markdown","metadata":{},"source":["> ## Predicting for invisual images:"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2022-05-15T11:46:11.279511Z","iopub.status.idle":"2022-05-15T11:46:11.280585Z"},"trusted":true},"outputs":[],"source":["def predict_img_class(img,model):\n","    \"\"\" Predict the class of image and Return Predicted Class\"\"\"\n","    img = to_device(img.unsqueeze(0), device)\n","    prediction =  model(img)\n","    _, preds = torch.max(prediction, dim = 1)\n","    return dataset.classes[preds[0].item()]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2022-05-15T11:46:11.282352Z","iopub.status.idle":"2022-05-15T11:46:11.283399Z"},"trusted":true},"outputs":[],"source":["from PIL import Image\n","\n","#open image file\n","img = Image.open(\"../input/intel-image-classification/seg_pred/seg_pred/10004.jpg\")\n","\n","#convert image to tensor\n","img = transforms.ToTensor()(img)\n","\n","#print image\n","plt.imshow(img.permute(1,2,0))\n","\n","#prdict image label\n","print(f\"Predicted Class : {predict_img_class(img,model)}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2022-05-15T11:46:11.285162Z","iopub.status.idle":"2022-05-15T11:46:11.286203Z"},"trusted":true},"outputs":[],"source":["#open image file\n","img = Image.open(\"../input/intel-image-classification/seg_pred/seg_pred/10100.jpg\")\n","\n","#convert image to tensor\n","img = transforms.ToTensor()(img)\n","\n","#print image\n","plt.imshow(img.permute(1,2,0))\n","\n","#prdict image label\n","print(f\"Predicted Class : {predict_img_class(img,model)}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2022-05-15T11:46:11.287951Z","iopub.status.idle":"2022-05-15T11:46:11.289008Z"},"trusted":true},"outputs":[],"source":["#open image file\n","img = Image.open(\"../input/intel-image-classification/seg_pred/seg_pred/10241.jpg\")\n","\n","#convert image to tensor\n","img = transforms.ToTensor()(img)\n","\n","#print image\n","plt.imshow(img.permute(1,2,0))\n","\n","#prdict image label\n","print(f\"Predicted Class : {predict_img_class(img,model)}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2022-05-15T11:46:11.29074Z","iopub.status.idle":"2022-05-15T11:46:11.291832Z"},"trusted":true},"outputs":[],"source":["img.shape"]},{"cell_type":"markdown","metadata":{},"source":["<!-- Save the parmeters to jovian plateform -->"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2022-05-15T11:46:11.293673Z","iopub.status.idle":"2022-05-15T11:46:11.294756Z"},"trusted":true},"outputs":[],"source":["# import jovian\n","\n","# jovian.log_dataset(dataset_url = data_dir, val_size = val_size, random_seed = random_seed)\n","\n","# #save the hyperparameters to jovian plateform \n","# #jovian.reset()\n","# jovian.log_hyperparams({\n","#     'num_epochs': num_epochs,\n","#     'opt_func': opt_func.__name__,\n","#     'batch_size': batch_size,\n","#     'lr': lr,\n","# })\n","\n","# jovian.log_metrics(test_loss=result['val_loss'], test_acc=result['val_acc'])\n","\n","# jovian.commit(project=project_name)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]}],"metadata":{"interpreter":{"hash":"366fffa44e6dbdb59f6ef3b9069558a28a03d278d01afa04fb6415cbc143bb28"},"kernelspec":{"display_name":"Python 3.9.12 ('venv': venv)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"}},"nbformat":4,"nbformat_minor":4}
