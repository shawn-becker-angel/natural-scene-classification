{"cells":[{"cell_type":"markdown","metadata":{},"source":[" **Note:** Igonre or comment jovian lines if you are running this notebook."]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2022-05-15T14:59:54.633875Z","iopub.status.busy":"2022-05-15T14:59:54.633166Z","iopub.status.idle":"2022-05-15T14:59:58.715978Z","shell.execute_reply":"2022-05-15T14:59:58.714806Z","shell.execute_reply.started":"2022-05-15T14:59:54.63383Z"},"trusted":true},"outputs":[],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","!python3 -m pip install --upgrade pip\n","%pip install torch\n","%pip install torchvision\n","%pip install matplotlib\n","\n","#import necessory libraries\n","import os\n","import json\n","import torch\n","import torchvision\n","import pandas as pd \n","import numpy as np\n","import matplotlib.pyplot as plt\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torchvision import transforms\n","from torchvision.datasets import ImageFolder\n","from torchvision.utils import make_grid\n","from torch.utils.data.dataloader import DataLoader\n","from torch.utils.data import random_split\n","%matplotlib inline\n","\n","print(\"done\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all \n","# files under the input directory\n","\n","seg_counts = {\n","    \"seg_test\": 0,\n","    \"seg_train\": 0,\n","    \"seg_pred\": 0\n","}\n","for dirname, _, filenames in os.walk('./kaggle/input'):\n","    num_files = len(filenames)\n","    if num_files > 0:\n","        for key in seg_counts.keys():\n","            if key in dirname:\n","                seg_counts[key] += num_files\n","\n","print(\"seg_counts:\\n\", json.dumps(seg_counts, indent=4))\n","\n","data_len = seg_counts['seg_train']\n","val_len = seg_counts['seg_test']\n","subsample = 10\n","print(\"old length of dataset:\", data_len)\n","index = range(0,data_len, subsample)\n","index_len = len(index)\n","val_len = round(val_len/subsample)\n","data_len = index_len\n","train_len = round(data_len - val_len)\n","print(\"index_len\", index_len, \"train_len\", train_len, \"val_len\", val_len)\n","\n","random_seed = 2021\n","torch.manual_seed(random_seed)\n","sampler = torch.utils.data.SubsetRandomSampler(index)"]},{"cell_type":"markdown","metadata":{},"source":["> # **Exploring the Dataset**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-15T11:28:18.074059Z","iopub.status.busy":"2022-05-15T11:28:18.073698Z","iopub.status.idle":"2022-05-15T11:28:18.079201Z","shell.execute_reply":"2022-05-15T11:28:18.078244Z","shell.execute_reply.started":"2022-05-15T11:28:18.074017Z"},"trusted":true},"outputs":[],"source":["#project name\n","project_name = 'natural-scene-classification'\n","\n","data_dir = \"./kaggle/input/seg_train/\"\n","test_data_dir = \"./kaggle/input/seg_test\"\n","pred_data_dir = \"./kaggle/input/seg_pred\""]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-15T11:28:18.095274Z","iopub.status.busy":"2022-05-15T11:28:18.094843Z","iopub.status.idle":"2022-05-15T11:28:18.384462Z","shell.execute_reply":"2022-05-15T11:28:18.383058Z","shell.execute_reply.started":"2022-05-15T11:28:18.095237Z"},"trusted":true},"outputs":[],"source":["\n","building_files = os.listdir(data_dir + '/buildings')\n","print(f\"Number of Buildings : {len(building_files)}\")\n","print(building_files[:5])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-15T11:28:18.38636Z","iopub.status.busy":"2022-05-15T11:28:18.386032Z","iopub.status.idle":"2022-05-15T11:28:31.882703Z","shell.execute_reply":"2022-05-15T11:28:31.881529Z","shell.execute_reply.started":"2022-05-15T11:28:18.386328Z"},"trusted":true},"outputs":[],"source":["transform = transforms.Compose([\n","    transforms.Resize((150,150)),\n","    transforms.ToTensor()\n","])\n","\n","dataset = ImageFolder(data_dir,transform=transform)\n","test_dataset = ImageFolder(test_data_dir,transform=transform)\n","\n","print(dataset)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-15T11:28:31.884376Z","iopub.status.busy":"2022-05-15T11:28:31.884093Z","iopub.status.idle":"2022-05-15T11:28:31.960478Z","shell.execute_reply":"2022-05-15T11:28:31.95934Z","shell.execute_reply.started":"2022-05-15T11:28:31.884345Z"},"trusted":true},"outputs":[],"source":["img, label = dataset[0]\n","print(img.shape,label)"]},{"cell_type":"markdown","metadata":{},"source":["The images are rgb and have 150*150 pixel in each image"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-15T11:28:31.963453Z","iopub.status.busy":"2022-05-15T11:28:31.963114Z","iopub.status.idle":"2022-05-15T11:28:31.969331Z","shell.execute_reply":"2022-05-15T11:28:31.968444Z","shell.execute_reply.started":"2022-05-15T11:28:31.963419Z"},"trusted":true},"outputs":[],"source":["print(f\"Images in dataset : {len(dataset)}\")\n","print(f\"Images in test dataset : {len(test_dataset)}\")"]},{"cell_type":"markdown","metadata":{},"source":["We have 14034 images in training data and 3000 images in test dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-15T11:28:31.971058Z","iopub.status.busy":"2022-05-15T11:28:31.97054Z","iopub.status.idle":"2022-05-15T11:28:31.982722Z","shell.execute_reply":"2022-05-15T11:28:31.981891Z","shell.execute_reply.started":"2022-05-15T11:28:31.971015Z"},"trusted":true},"outputs":[],"source":["print(\"dataset classes: \\n\",dataset.classes)\n","print(\"test_dataset classes: \\n\",test_dataset.classes)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-15T11:28:31.984434Z","iopub.status.busy":"2022-05-15T11:28:31.984005Z","iopub.status.idle":"2022-05-15T11:28:31.992422Z","shell.execute_reply":"2022-05-15T11:28:31.991509Z","shell.execute_reply.started":"2022-05-15T11:28:31.984399Z"},"trusted":true},"outputs":[],"source":["def display_img(img,label):\n","    print(f\"Label : {dataset.classes[label]}\")\n","    plt.imshow(img.permute(1,2,0))"]},{"cell_type":"markdown","metadata":{},"source":["> # Visualising some images :"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-15T11:28:31.994197Z","iopub.status.busy":"2022-05-15T11:28:31.9937Z","iopub.status.idle":"2022-05-15T11:28:32.227772Z","shell.execute_reply":"2022-05-15T11:28:32.226791Z","shell.execute_reply.started":"2022-05-15T11:28:31.994157Z"},"trusted":true},"outputs":[],"source":["display_img(*dataset[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-15T11:28:32.229621Z","iopub.status.busy":"2022-05-15T11:28:32.2293Z","iopub.status.idle":"2022-05-15T11:28:32.440555Z","shell.execute_reply":"2022-05-15T11:28:32.439435Z","shell.execute_reply.started":"2022-05-15T11:28:32.229589Z"},"trusted":true},"outputs":[],"source":["display_img(*dataset[5000])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-15T11:28:32.443025Z","iopub.status.busy":"2022-05-15T11:28:32.442553Z","iopub.status.idle":"2022-05-15T11:28:32.644801Z","shell.execute_reply":"2022-05-15T11:28:32.643956Z","shell.execute_reply.started":"2022-05-15T11:28:32.442971Z"},"trusted":true},"outputs":[],"source":["display_img(*dataset[8000])"]},{"cell_type":"markdown","metadata":{},"source":["> # Training and Validation Datasets : "]},{"cell_type":"markdown","metadata":{},"source":["> # Load the dataset into batches:"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-15T11:28:32.685347Z","iopub.status.busy":"2022-05-15T11:28:32.684818Z","iopub.status.idle":"2022-05-15T11:28:32.697062Z","shell.execute_reply":"2022-05-15T11:28:32.696044Z","shell.execute_reply.started":"2022-05-15T11:28:32.685309Z"},"trusted":true},"outputs":[],"source":["train_dl = DataLoader(dataset, sampler=sampler, batch_size=4)\n","test_dl = DataLoader(test_dataset, sampler=sampler, batch_size=4)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-15T11:28:32.698887Z","iopub.status.busy":"2022-05-15T11:28:32.698548Z","iopub.status.idle":"2022-05-15T11:28:32.7081Z","shell.execute_reply":"2022-05-15T11:28:32.707152Z","shell.execute_reply.started":"2022-05-15T11:28:32.698853Z"},"trusted":true},"outputs":[],"source":["from torchvision.utils import make_grid\n","\n","def show_batch(dl):\n","    for images, labels in dl:\n","        fig,ax = plt.subplots(figsize = (16,12))\n","        ax.set_xticks([])\n","        ax.set_yticks([])\n","        ax.imshow(make_grid(images,nrow=16).permute(1,2,0))\n","        break"]},{"cell_type":"markdown","metadata":{},"source":["> # **Grid Of Train Data Images :**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-15T11:28:32.71002Z","iopub.status.busy":"2022-05-15T11:28:32.709623Z","iopub.status.idle":"2022-05-15T11:28:35.586299Z","shell.execute_reply":"2022-05-15T11:28:35.584675Z","shell.execute_reply.started":"2022-05-15T11:28:32.709983Z"},"trusted":true},"outputs":[],"source":["show_batch(train_dl)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["show_batch(test_dl)"]},{"cell_type":"markdown","metadata":{},"source":["> # Base Model for Image Classification:"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-15T11:28:35.589076Z","iopub.status.busy":"2022-05-15T11:28:35.588567Z","iopub.status.idle":"2022-05-15T11:28:35.609517Z","shell.execute_reply":"2022-05-15T11:28:35.608037Z","shell.execute_reply.started":"2022-05-15T11:28:35.589023Z"},"trusted":true},"outputs":[],"source":["class ImageClassificationBase(nn.Module):\n","    \n","    def training_step(self, batch):\n","        images, labels = batch \n","        out = self(images)                  # Generate predictions\n","        loss = F.cross_entropy(out, labels) # Calculate loss\n","        return loss\n","    \n","    def validation_step(self, batch):\n","        images, labels = batch \n","        out = self(images)                    # Generate predictions\n","        loss = F.cross_entropy(out, labels)   # Calculate loss\n","        acc = accuracy(out, labels)           # Calculate accuracy\n","        return {'val_loss': loss.detach(), 'val_acc': acc}\n","        \n","    def validation_epoch_end(self, outputs):\n","        batch_losses = [x['val_loss'] for x in outputs]\n","        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n","        batch_accs = [x['val_acc'] for x in outputs]\n","        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n","        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n","    \n","    def epoch_end(self, epoch, result):\n","        print(\"Epoch [{}], train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n","            epoch, result['train_loss'], result['val_loss'], result['val_acc']))\n","        \n","def accuracy(outputs, labels):\n","    _, preds = torch.max(outputs, dim=1)\n","    return torch.tensor(torch.sum(preds == labels).item() / len(preds))"]},{"cell_type":"markdown","metadata":{},"source":["# Netural Scene Classfication Model:"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-15T11:28:35.611253Z","iopub.status.busy":"2022-05-15T11:28:35.610796Z","iopub.status.idle":"2022-05-15T11:28:35.626472Z","shell.execute_reply":"2022-05-15T11:28:35.625552Z","shell.execute_reply.started":"2022-05-15T11:28:35.611216Z"},"trusted":true},"outputs":[],"source":["class NaturalSceneClassification(ImageClassificationBase):\n","    \n","    def __init__(self):\n","        \n","        super().__init__()\n","        self.network = nn.Sequential(\n","            \n","            nn.Conv2d(3, 32, kernel_size = 3, padding = 1),\n","            nn.ReLU(),\n","            nn.Conv2d(32,64, kernel_size = 3, stride = 1, padding = 1),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2,2),\n","        \n","            nn.Conv2d(64, 128, kernel_size = 3, stride = 1, padding = 1),\n","            nn.ReLU(),\n","            nn.Conv2d(128 ,128, kernel_size = 3, stride = 1, padding = 1),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2,2),\n","            \n","            nn.Conv2d(128, 256, kernel_size = 3, stride = 1, padding = 1),\n","            nn.ReLU(),\n","            nn.Conv2d(256,256, kernel_size = 3, stride = 1, padding = 1),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2,2),\n","            \n","            nn.Flatten(),\n","            nn.Linear(82944,1024),\n","            nn.ReLU(),\n","            nn.Linear(1024, 512),\n","            nn.ReLU(),\n","            nn.Linear(512,6)\n","        )\n","    \n","    def forward(self, xb):\n","        return self.network(xb)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-15T11:28:35.62807Z","iopub.status.busy":"2022-05-15T11:28:35.627654Z","iopub.status.idle":"2022-05-15T11:28:36.445817Z","shell.execute_reply":"2022-05-15T11:28:36.444543Z","shell.execute_reply.started":"2022-05-15T11:28:35.62803Z"},"trusted":true},"outputs":[],"source":["model = NaturalSceneClassification()\n","model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-15T11:28:36.448471Z","iopub.status.busy":"2022-05-15T11:28:36.447983Z","iopub.status.idle":"2022-05-15T11:28:50.582271Z","shell.execute_reply":"2022-05-15T11:28:50.581126Z","shell.execute_reply.started":"2022-05-15T11:28:36.448421Z"},"trusted":true},"outputs":[],"source":["for images, labels in train_dl:\n","    print('images.shape:', images.shape)\n","    out = model(images)\n","    print('out.shape:', out.shape)\n","    print('out[0]:', out[0])\n","    break"]},{"cell_type":"markdown","metadata":{},"source":["Helper Function or classes to Load Data into GPU"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-15T11:28:50.584704Z","iopub.status.busy":"2022-05-15T11:28:50.584118Z","iopub.status.idle":"2022-05-15T11:28:50.599699Z","shell.execute_reply":"2022-05-15T11:28:50.598438Z","shell.execute_reply.started":"2022-05-15T11:28:50.584656Z"},"trusted":true},"outputs":[],"source":["def get_default_device():\n","    \"\"\" Set Device to GPU or CPU\"\"\"\n","    if torch.cuda.is_available():\n","        return torch.device('cuda')\n","    else:\n","        return torch.device('cpu')\n","    \n","\n","def to_device(data, device):\n","    \"Move data to the device\"\n","    if isinstance(data,(list,tuple)):\n","        return [to_device(x,device) for x in data]\n","    return data.to(device,non_blocking = True)\n","\n","class DeviceDataLoader():\n","    \"\"\" Wrap a dataloader to move data to a device \"\"\"\n","    \n","    def __init__(self, dl, device):\n","        self.dl = dl\n","        self.device = device\n","    \n","    def __iter__(self):\n","        \"\"\" Yield a batch of data after moving it to device\"\"\"\n","        for b in self.dl:\n","            yield to_device(b,self.device)\n","            \n","    def __len__(self):\n","        \"\"\" Number of batches \"\"\"\n","        return len(self.dl)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-15T11:28:50.602259Z","iopub.status.busy":"2022-05-15T11:28:50.601524Z","iopub.status.idle":"2022-05-15T11:28:50.621706Z","shell.execute_reply":"2022-05-15T11:28:50.620662Z","shell.execute_reply.started":"2022-05-15T11:28:50.60221Z"},"trusted":true},"outputs":[],"source":["device = get_default_device()\n","device"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-15T11:28:50.624224Z","iopub.status.busy":"2022-05-15T11:28:50.623417Z","iopub.status.idle":"2022-05-15T11:28:50.638976Z","shell.execute_reply":"2022-05-15T11:28:50.637798Z","shell.execute_reply.started":"2022-05-15T11:28:50.624165Z"},"trusted":true},"outputs":[],"source":["# load the into GPU\n","train_dl = DeviceDataLoader(train_dl, device)\n","val_dl = DeviceDataLoader(val_dl, device)\n","to_device(model, device)"]},{"cell_type":"markdown","metadata":{},"source":["> # **Model Fitting**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-15T11:28:50.641577Z","iopub.status.busy":"2022-05-15T11:28:50.640816Z","iopub.status.idle":"2022-05-15T11:28:50.655246Z","shell.execute_reply":"2022-05-15T11:28:50.654023Z","shell.execute_reply.started":"2022-05-15T11:28:50.641525Z"},"trusted":true},"outputs":[],"source":["@torch.no_grad()\n","def evaluate(model, val_loader):\n","    model.eval()\n","    outputs = [model.validation_step(batch) for batch in val_loader]\n","    return model.validation_epoch_end(outputs)\n","\n","def fit(epochs, lr, model, train_loader, val_loader, opt_func = torch.optim.SGD):\n","    \n","    history = []\n","    optimizer = opt_func(model.parameters(),lr)\n","    for epoch in range(epochs):\n","        \n","        model.train()\n","        train_losses = []\n","        for batch in train_loader:\n","            loss = model.training_step(batch)\n","            train_losses.append(loss)\n","            loss.backward()\n","            optimizer.step()\n","            optimizer.zero_grad()\n","            \n","        result = evaluate(model, val_loader)\n","        result['train_loss'] = torch.stack(train_losses).mean().item()\n","        model.epoch_end(epoch, result)\n","        history.append(result)\n","    \n","    return history"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-15T11:28:50.663723Z","iopub.status.busy":"2022-05-15T11:28:50.66275Z","iopub.status.idle":"2022-05-15T11:28:51.472451Z","shell.execute_reply":"2022-05-15T11:28:51.471301Z","shell.execute_reply.started":"2022-05-15T11:28:50.663665Z"},"trusted":true},"outputs":[],"source":["#load the model to the device\n","model = to_device(NaturalSceneClassification(),device)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-15T11:28:51.474617Z","iopub.status.busy":"2022-05-15T11:28:51.474076Z","iopub.status.idle":"2022-05-15T11:31:47.993531Z","shell.execute_reply":"2022-05-15T11:31:47.992369Z","shell.execute_reply.started":"2022-05-15T11:28:51.474578Z"},"trusted":true},"outputs":[],"source":["#initial evaluation of the model\n","evaluate(model,val_dl)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-15T11:31:47.995685Z","iopub.status.busy":"2022-05-15T11:31:47.995314Z","iopub.status.idle":"2022-05-15T11:31:48.000538Z","shell.execute_reply":"2022-05-15T11:31:47.999635Z","shell.execute_reply.started":"2022-05-15T11:31:47.995641Z"},"trusted":true},"outputs":[],"source":["#set the no. of epochs, optimizer funtion and learning rate\n","num_epochs = 30\n","opt_func = torch.optim.Adam\n","lr = 0.001"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-15T11:31:48.002705Z","iopub.status.busy":"2022-05-15T11:31:48.002182Z","iopub.status.idle":"2022-05-15T11:46:11.265813Z","shell.execute_reply":"2022-05-15T11:46:11.263234Z","shell.execute_reply.started":"2022-05-15T11:31:48.002667Z"},"trusted":true},"outputs":[],"source":["#fitting the model on training data and record the result after each epoch\n","history = fit(num_ep\n","              \n","              \n","              \n","              ochs, lr, model, train_dl, val_dl, opt_func)"]},{"cell_type":"markdown","metadata":{},"source":["> # Graphs for Model Accuracy and Losses :"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2022-05-15T11:46:11.268032Z","iopub.status.idle":"2022-05-15T11:46:11.268836Z"},"trusted":true},"outputs":[],"source":["def plot_accuracies(history):\n","    \"\"\" Plot the history of accuracies\"\"\"\n","    accuracies = [x['val_acc'] for x in history]\n","    plt.plot(accuracies, '-x')\n","    plt.xlabel('epoch')\n","    plt.ylabel('accuracy')\n","    plt.title('Accuracy vs. No. of epochs');\n","    \n","\n","plot_accuracies(history)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2022-05-15T11:46:11.270823Z","iopub.status.idle":"2022-05-15T11:46:11.271855Z"},"trusted":true},"outputs":[],"source":["def plot_losses(history):\n","    \"\"\" Plot the losses in each epoch\"\"\"\n","    train_losses = [x.get('train_loss') for x in history]\n","    val_losses = [x['val_loss'] for x in history]\n","    plt.plot(train_losses, '-bx')\n","    plt.plot(val_losses, '-rx')\n","    plt.xlabel('epoch')\n","    plt.ylabel('loss')\n","    plt.legend(['Training', 'Validation'])\n","    plt.title('Loss vs. No. of epochs');\n","\n","plot_losses(history)"]},{"cell_type":"markdown","metadata":{},"source":["> # Evaluate Test Data :"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2022-05-15T11:46:11.273726Z","iopub.status.idle":"2022-05-15T11:46:11.27482Z"},"trusted":true},"outputs":[],"source":["# Apply the model on test dataset and Get the results\n","test_loader = DeviceDataLoader(DataLoader(test_dataset, batch_size*2), device)\n","result = evaluate(model, test_loader)\n","result"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2022-05-15T11:46:11.276627Z","iopub.status.idle":"2022-05-15T11:46:11.277687Z"},"trusted":true},"outputs":[],"source":["#save the model\n","torch.save(model.state_dict(), 'natural-scene-classification.pth')"]},{"cell_type":"markdown","metadata":{},"source":["> ## Predicting for individual images:"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2022-05-15T11:46:11.279511Z","iopub.status.idle":"2022-05-15T11:46:11.280585Z"},"trusted":true},"outputs":[],"source":["def predict_img_class(img,model):\n","    \"\"\" Predict the class of image and Return Predicted Class\"\"\"\n","    img = to_device(img.unsqueeze(0), device)\n","    prediction =  model(img)\n","    _, preds = torch.max(prediction, dim = 1)\n","    return dataset.classes[preds[0].item()]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2022-05-15T11:46:11.282352Z","iopub.status.idle":"2022-05-15T11:46:11.283399Z"},"trusted":true},"outputs":[],"source":["from PIL import Image\n","\n","#open image file\n","img = Image.open(\"./kaggle/input/seg_pred/10004.jpg\")\n","\n","#convert image to tensor\n","img = transforms.ToTensor()(img)\n","\n","#print image\n","plt.imshow(img.permute(1,2,0))\n","\n","#prdict image label\n","print(f\"Predicted Class : {predict_img_class(img,model)}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2022-05-15T11:46:11.285162Z","iopub.status.idle":"2022-05-15T11:46:11.286203Z"},"trusted":true},"outputs":[],"source":["#open image file\n","img = Image.open(\"./kaggle/input/seg_pred/10100.jpg\")\n","\n","#convert image to tensor\n","img = transforms.ToTensor()(img)\n","\n","#print image\n","plt.imshow(img.permute(1,2,0))\n","\n","#prdict image label\n","print(f\"Predicted Class : {predict_img_class(img,model)}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2022-05-15T11:46:11.287951Z","iopub.status.idle":"2022-05-15T11:46:11.289008Z"},"trusted":true},"outputs":[],"source":["#open image file\n","img = Image.open(\"./kaggle/input/seg_pred/10241.jpg\")\n","\n","#convert image to tensor\n","img = transforms.ToTensor()(img)\n","\n","#print image\n","plt.imshow(img.permute(1,2,0))\n","\n","#prdict image label\n","print(f\"Predicted Class : {predict_img_class(img,model)}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2022-05-15T11:46:11.29074Z","iopub.status.idle":"2022-05-15T11:46:11.291832Z"},"trusted":true},"outputs":[],"source":["img.shape"]},{"cell_type":"markdown","metadata":{},"source":["<!-- Save the parmeters to jovian plateform -->"]}],"metadata":{"interpreter":{"hash":"366fffa44e6dbdb59f6ef3b9069558a28a03d278d01afa04fb6415cbc143bb28"},"kernelspec":{"display_name":"Python 3.9.12 ('venv': venv)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"}},"nbformat":4,"nbformat_minor":4}
